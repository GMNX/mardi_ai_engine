{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mardi Inference Server Logic\n",
    "\n",
    "## Diagram\n",
    "![alt](../images/diagram.png)\n",
    "\n",
    "## Introduction\n",
    "This notebook presents an advanced methodology for processing, classifying, and segmenting images using state-of-the-art computer vision and deep learning techniques. The project implements cutting-edge models such as *You Only Look Once (YOLO)* and *Segment Anything Model (SAM)*, achieving\n",
    "a comprehensive and efficient workflow that allows for precise image analysis for detection and segmentation.\n",
    "\n",
    "An important aspect of this project was the addition of noise to images to test the model's robustness. Noise refers to random disturbances in a signal, and in our case, the signal was an image. Random disturbances in the brightness and color of an image are called image noise. Specifically, we introduced *salt-and-pepper noise*, which is a type of impulse noise found only in grayscale images.\n",
    "\n",
    "Finally, we utilized a *Random Forest* classification model to predict the week of planting based on features extracted from the segmented images. *Random Forest* is an ensemble learning method known for its high accuracy and ability to handle many input features.\n",
    "\n",
    "The methodology employed in this project demonstrated a comprehensive solution for image processing, classification, segmentation, and annotation using state-of-the-art deep learning and computer vision techniques. By integrating *You Only Look Once (YOLO)* for detection, *Segment Anything Model (SAM)* for segmentation, and *Random Forest* for prediction, the workflow ensured high precision and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The first step involved preparing the images for analysis. All images were resized to a standard\n",
    "dimension of 224x224 pixels. This standardization ensured that each image input into the model had\n",
    "consistent dimensions, thereby enhancing the model's accuracy and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read and process the image\n",
    "image_path = '../images/week_2.jpg'\n",
    "original_image = cv2.imread(image_path)\n",
    "resized_image = cv2.resize(original_image, (224, 224))\n",
    "\n",
    "# Convert BGR to RGB\n",
    "resized_image_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(resized_image_rgb)\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.title(os.path.basename(image_path))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age Group Classification using YOLO\n",
    "Image classification involved assigning an entire image to one of a set of predefined classes. The output of an image classifier was a single class label accompanied by a confidence score. This task was fundamental when the objective was to determine the overall category to which an image belonged, without needing to specify the locations or shapes of individual objects within the image. The *You Only Look Once (YOLO)* model was utilized for this purpose due to its efficiency and accuracy in real-time applications. The images were classified into two classes based on the following criteria:\n",
    "• *Class 1*: Include images of crops at 1, 2, and 3 weeks of age.\n",
    "• *Class 2*: Include images of crops at 4, and 5 weeks of age.\n",
    "The training process involved using the *YOLOv8* model for classification. The training process for the YOLO model was configured with several parameters to optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.general import check_img_size\n",
    "from utils.torch_utils import select_device\n",
    "\n",
    "device = select_device('0')\n",
    "\n",
    "age_group_weights = '/data/models/model_classification.onnx'\n",
    "age_group_model = DetectMultiBackend(age_group_weights, device=device, dnn=False, data='data/coco128.yaml', fp16=False)\n",
    "age_group_stride = age_group_model.stride\n",
    "age_group_imgsz = check_img_size((224, 224), s=age_group_stride)  # check image size\n",
    "age_group_model.warmup(imgsz=(1, 3, *age_group_imgsz))\n",
    "\n",
    "def preprocess_image_yolo_classification(image: np.ndarray) -> np.ndarray:\n",
    "    im = image.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "    im = np.ascontiguousarray(im)  # contiguous\n",
    "    im = torch.from_numpy(im).to(age_group_model.device)\n",
    "    im = im.half() if age_group_model.fp16 else im.float()  # uint8 to fp16/32\n",
    "    im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # expand for batch dim\n",
    "    return im\n",
    "\n",
    "im = preprocess_image_yolo_classification(resized_image)\n",
    "results = age_group_model(im)\n",
    "top_class = results.argmax(dim=1).item()\n",
    "age_group = age_group_model.names[top_class]\n",
    "\n",
    "print(f'Predicted age group: {age_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection using YOLO\n",
    "Object detection was performed using the *YOLOv9* model, specifically targeting the detection of plants within the images classified as *Class 1*. This process was automated, utilizing the pre-trained model *\"gelan-c.pt\"*. The use of this model allowed for high-resolution real-time detection without the need for additional training or manual annotations.\n",
    "\n",
    "The model was specifically designed to automatically annotate the images using the *COCO (Common Objects in Context)* dataset. The COCO dataset is a large-scale object detection, segmentation, and captioning dataset that is widely used for benchmarking computer vision models. It includes a diverse set of object categories and provides high-quality annotations for research in object detection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from utils.augmentations import letterbox\n",
    "from utils.general import non_max_suppression, scale_boxes, xyxy2xywh\n",
    "\n",
    "# Define the desired classes\n",
    "desired_classes = [25, 58]\n",
    "\n",
    "# Random color map for each class\n",
    "color_map = {}\n",
    "for class_id in desired_classes:\n",
    "    color_map[class_id] = tuple(np.random.randint(0, 256, 3).tolist())\n",
    "\n",
    "conf_thres = 0.1\n",
    "iou_thres = 0.45\n",
    "classes = None\n",
    "agnostic_nms = False\n",
    "max_det = 1000\n",
    "\n",
    "object_detection_weights = '/data/yolov9/weights/gelan-c.pt'\n",
    "object_detection_model = DetectMultiBackend(object_detection_weights, device=device, dnn=False, data='data/coco.yaml', fp16=False)\n",
    "object_detection_stride, object_detection_pt = object_detection_model.stride, object_detection_model.pt\n",
    "object_detection_imgsz = check_img_size((640, 640), s=object_detection_stride)  # check image size\n",
    "object_detection_model.warmup(imgsz=(1 if object_detection_pt or object_detection_model.triton else 1, 3, *object_detection_imgsz))\n",
    "\n",
    "def preprocess_image_object_detection(self, image: np.ndarray) -> np.ndarray:\n",
    "    im = letterbox(image, 640, stride=stride_yolov9, auto=True)[0]  # padded resize\n",
    "    im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "    im = np.ascontiguousarray(im)  # contiguous\n",
    "    im = torch.from_numpy(im).to(yolov9_model.device)\n",
    "    im = im.half() if yolov9_model.fp16 else im.float()  # uint8 to fp16/32\n",
    "    im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # expand for batch dim\n",
    "    return im\n",
    "\n",
    "def show_box(image, box, label, conf_score, color):\n",
    "    x0, y0 = int(box[0]), int(box[1])\n",
    "    x1, y1 = int(box[2]), int(box[3])\n",
    "    cv2.rectangle(image, (x0, y0), (x1, y1), color, 2)\n",
    "    label_text = f'{label} {conf_score:.2f}'\n",
    "    cv2.putText(image, label_text, (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "# Load class names from YAML file\n",
    "with open('data/coco.yaml', 'r') as file:\n",
    "    coco_data = yaml.safe_load(file)\n",
    "    class_names = coco_data['names']\n",
    "\n",
    "class_ids = []\n",
    "bboxes = []\n",
    "conf_scores = []\n",
    "\n",
    "if age_group == \"Class1\":\n",
    "    im = preprocess_image_object_detection(resized_image)\n",
    "    pred = object_detection_model(im, augment=False, visualize=False)\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "\n",
    "    image_height, image_width, _ = resized_image.shape\n",
    "    gn = torch.tensor(resized_image.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "    aggregate_mask = np.zeros(resized_image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    # Process predictions\n",
    "    for i, det in enumerate(pred):\n",
    "        if len(det):\n",
    "            # Rescale boxes from img_size to image size\n",
    "            det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], resized_image.shape).round()\n",
    "\n",
    "            # Write results\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                if not cls in desired_classes:\n",
    "                    continue\n",
    "\n",
    "                xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                cx, cy, w, h = xywh\n",
    "\n",
    "                # Convert from normalized [0, 1] to image scale\n",
    "                cx *= image_width\n",
    "                cy *= image_height\n",
    "                w *= image_width\n",
    "                h *= image_height\n",
    "\n",
    "                # Convert center x, y, width and height to xmin, ymin, xmax, ymax\n",
    "                xmin = cx - w / 2\n",
    "                ymin = cy - h / 2\n",
    "                xmax = cx + w / 2\n",
    "                ymax = cy + h / 2\n",
    "\n",
    "                class_ids.append(cls)\n",
    "                bboxes.append((xmin, ymin, xmax, ymax))\n",
    "                conf_scores.append(conf)\n",
    "\n",
    "# Create a copy of the image to draw bounding boxes\n",
    "image_with_bboxes = resized_image.copy()\n",
    "\n",
    "# Draw bounding boxes\n",
    "for class_id, bbox, conf_score in zip(class_ids, bboxes, conf_scores):\n",
    "    class_name = class_names[class_id]\n",
    "    color = color_map[class_id]\n",
    "    show_box(image_with_bboxes, bbox, class_name, conf_score, color)\n",
    "\n",
    "# Convert BGR to RGB for displaying with matplotlib\n",
    "image_with_bboxes_rgb = cv2.cvtColor(image_with_bboxes, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(image_with_bboxes_rgb)\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.title('Image with Bounding Boxes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Segmentation using SAM\n",
    "First, *SAM* was used to extract masks from the detections made by *YOLO* in *Class 1*. This stage was crucial for isolating the objects detected by *YOLO* and preparing them for more detailed segmentation. We called it *SAM1* for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "model_type = \"vit_h\"\n",
    "sam_checkpoint = \"/data/models/sam_vit_h_4b8939.pth\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam_predictor_model = SamPredictor(sam)\n",
    "\n",
    "# Find the most central bounding box if there are multiple detections\n",
    "def find_central_bbox(bboxes):\n",
    "    center_x, center_y = image_width / 2, image_height / 2\n",
    "    min_distance = float('inf')\n",
    "    central_bbox = None\n",
    "    central_class_id = None\n",
    "    central_conf_score = None\n",
    "\n",
    "    for class_id, bbox, conf_score in zip(class_ids, bboxes, conf_scores):\n",
    "        if class_id in desired_classes:\n",
    "            bbox_center_x = (bbox[0] + bbox[2]) / 2\n",
    "            bbox_center_y = (bbox[1] + bbox[3]) / 2\n",
    "            distance = np.sqrt((center_x - bbox_center_x) ** 2 + (center_y - bbox_center_y) ** 2)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                central_bbox = bbox\n",
    "                central_class_id = class_id\n",
    "                central_conf_score = conf_score\n",
    "\n",
    "    return central_class_id, central_bbox, central_conf_score\n",
    "\n",
    "\n",
    "# Get the most central bounding box\n",
    "central_class_id, central_bbox, central_conf_score = find_central_bbox(bboxes)\n",
    "\n",
    "if central_class_id is not None and central_bbox is not None:\n",
    "    # Generate and accumulate masks for each bounding box\n",
    "    class_name = class_names[central_class_id]\n",
    "    color = color_map[central_class_id]\n",
    "\n",
    "    image_with_central_bbox = resized_image.copy()\n",
    "    show_box(image_with_central_bbox, central_bbox, class_name, central_conf_score, color)\n",
    "    image_with_central_bbox_rgb = cv2.cvtColor(image_with_central_bbox, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(image_with_central_bbox_rgb)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.title('Image with Central Bounding Box')\n",
    "    plt.show()\n",
    "\n",
    "    # Generate mask for the central bounding box\n",
    "    input_box = np.array(central_bbox).reshape(1, 4)\n",
    "    masks, _, _ = sam_predictor_model.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_box,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    aggregate_mask = np.where(masks[0] > 0.5, 1, aggregate_mask)\n",
    "\n",
    "    # Convert the aggregated segmentation mask to a binary mask\n",
    "    binary_mask = np.where(aggregate_mask == 1, 1, 0)\n",
    "\n",
    "    # Create a white background with the same size as the image\n",
    "    white_background = np.ones_like(resized_image) * 255\n",
    "\n",
    "    # Applying the binary mask to the original image\n",
    "    # Where the binary mask is 0 (background), use white background; otherwise, use the original image.\n",
    "    segmented_image = white_background * (1 - binary_mask[..., np.newaxis]) + resized_image * binary_mask[..., np.newaxis]\n",
    "\n",
    "    # Replace resized_image with segmented_image for the next steps\n",
    "    resized_image = segmented_image.astype(np.uint8)\n",
    "\n",
    "    # Show the new image\n",
    "    segmented_image_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(segmented_image_rgb)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.title('Segmented Image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Noise and Processing\n",
    "*Salt-and-pepper noise* was added to the images to test the model's robustness. Noise referred to random disturbances in a signal, and in our case, the signal was an image. *Salt-and-pepper noise* is a type of impulse noise found only in grayscale images, introducing white spots in dark regions and black spots in light regions. Adding this noise simulated real-world conditions where images are not always perfect, ensuring that the model remained robust and reliable across various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mardi import add_noise\n",
    "\n",
    "gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "noisy_img = add_noise(gray_image)\n",
    "\n",
    "image_bgr = cv2.cvtColor(noisy_img, cv2.COLOR_GRAY2BGR)\n",
    "image_rgb = cv2.cvtColor(noisy_img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.title('Noisy Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction and Classification\n",
    "After extracting the initial masks with *SAM1*, an additional *SAM* was applied to perform more detailed and precise segmentation of objects in both classes (*Class 1* and *Class 2*). This second stage allowed for automatic and detailed segmentation. We called it *SAM2* for this process.\n",
    "\n",
    "Feature extraction involved calculating specific characteristics from the segmented images, which were then used as input features for the Random Forest model. The primary features extracted were the number of segments (count) and the area of each segment. For each image, the following statistical measures of the segment areas were calculated:\n",
    "• *Standard Deviation of Area (area_std)*: This measure provided insights into the variability of segment sizes within an image.\n",
    "• *Mean of Area (area_mean)*: This measure indicates the average size of the segments.\n",
    "• *Median of Area (area_median)*: This measure highlighted the central tendency of the segment sizes, less affected by outliers.\n",
    "These features were essential for capturing the distribution and variability of plant segment sizes within the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import onnxruntime as rt\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "from utils.mardi import filter_out_background, generate_colors, is_white_background\n",
    "\n",
    " # Classification Group 1 model\n",
    "classification1_model_path = \"/data/models/rf_model_class1.onnx\"\n",
    "classification1_model = rt.InferenceSession(classification1_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "classification1_input_name = classification1_model.get_inputs()[0].name\n",
    "classification1_label_name = classification1_model.get_outputs()[0].name\n",
    "\n",
    "# Classification Group 2 model\n",
    "classification2_model_path = \"/data/models/rf_model_class2.onnx\"\n",
    "classification2_model = rt.InferenceSession(classification2_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "classification2_input_name = classification2_model.get_inputs()[0].name\n",
    "classification2_label_name = classification2_model.get_outputs()[0].name\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "sam_result = mask_generator.generate(image_rgb)\n",
    "\n",
    "# Filter out the background\n",
    "filtered_masks = filter_out_background(sam_result)\n",
    "\n",
    "annotated_image = image_bgr.copy()\n",
    "colors = generate_colors(len(filtered_masks))\n",
    "\n",
    "segment_count = 0\n",
    "mask_data = []\n",
    "for i, mask in enumerate(filtered_masks):\n",
    "    area = mask['area']\n",
    "    segmentation = mask['segmentation'].astype('uint8')\n",
    "\n",
    "    # Skip white background areas\n",
    "    if is_white_background(segmentation, image_rgb):\n",
    "        continue\n",
    "\n",
    "    # Color the segmented area\n",
    "    color = colors[i]\n",
    "    r, g, b = color\n",
    "    annotated_image[segmentation > 0] = cv2.addWeighted(annotated_image, 0.5, np.full_like(annotated_image, color), 0.5, 0)[segmentation > 0]\n",
    "\n",
    "    # Append data\n",
    "    segment_count += 1\n",
    "    mask_data.append([area, r, g, b])\n",
    "\n",
    "columns = ['area', 'r', 'g', 'b']\n",
    "data = pd.DataFrame(mask_data, columns=columns)\n",
    "\n",
    "# Create derived features, handling potential division by zero\n",
    "data['r/g'] = (data['r'] / (data['g'] + 1e-8)).round(4)  # Add a small constant to avoid division by zero and round to 4 decimals\n",
    "data['r/b'] = (data['r'] / (data['b'] + 1e-8)).round(4)\n",
    "data['g/b'] = (data['g'] / (data['b'] + 1e-8)).round(4)\n",
    "\n",
    "# Calculate aggregated statistics for 'area'\n",
    "agg_area = data['area'].agg(['mean', 'median', 'std']).reset_index()\n",
    "agg_area.columns = ['statistic', 'value']\n",
    "\n",
    "# Round aggregated statistics to 4 decimal places\n",
    "area_mean = agg_area.loc[agg_area['statistic'] == 'mean', 'value'].round(4).values[0]\n",
    "area_median = agg_area.loc[agg_area['statistic'] == 'median', 'value'].round(4).values[0]\n",
    "area_std = agg_area.loc[agg_area['statistic'] == 'std', 'value'].round(4).values[0]\n",
    "\n",
    "classification_input = np.array([[segment_count, area_std, area_mean, area_median]]).astype(np.float32)\n",
    "\n",
    "if age_group == \"Class1\":\n",
    "    age = classification1_model.run([classification1_label_name], {classification1_input_name: classification_input})[0]\n",
    "else:\n",
    "    age = classification2_model.run([classification2_label_name], {classification2_input_name: classification_input})[0]\n",
    "\n",
    "if len(age) > 0:\n",
    "    age = age[0]\n",
    "else:\n",
    "    age = None\n",
    "\n",
    "print(f\"Segment count: {segment_count}\")\n",
    "print(f\"Age: {age}\")\n",
    "\n",
    "# Display the annotated image\n",
    "annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(annotated_image_rgb)\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.title('Annotated Image')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
